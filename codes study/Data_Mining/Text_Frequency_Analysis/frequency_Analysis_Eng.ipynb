{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### 데이터 수집\n",
    "\n",
    "1) 한국교육학술정보원(KERIS)의 학술연구정보서비스(RISS) 사이트 이용\n",
    "    - 1,000개의 해외 학술 문서 제목을 다운로드해 사용\n",
    "    - 링크: (http://www.riss.kr/index.do)\n",
    "2) 통합검색 결과 페이지에서 [해외학술논문] 메뉴를 클릭\n",
    "    - ‘Big data’로 검색한 결과\n",
    "        - ‘해외학술논문’ 선택 → 17,597 건의 결과 확인(2024.10.30일 기준)\n",
    "3) ‘작성언어’를 [영어]로 선택하고 ‘실행’ 버튼을 클릭\n",
    "4) 검색 결과 출력 개수 변경하기\n",
    "    - ‘100개씩 출력’ → 조회\n",
    "5) 조회된 검색 결과를 선택하고 내보내기 버튼 선택\n",
    "6) [Excel저장]을 선택 → 버튼을 클릭한 뒤 파일을 \n",
    "7) 다음 페이지로 이동하여 이전 과정을 반복\n",
    "8) 다운로드한 총 10개의 엑셀 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS, WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('dataSet/exportExcelData_*.xls')\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data = [] #저장할 리스트\n",
    "for file in all_files: \n",
    "    data_frame = pd.read_excel(file) \n",
    "    all_files_data.append(data_frame) \n",
    "all_files_data[0] #작업 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat = pd.concat(all_files_data, axis = 0, ignore_index = True) \n",
    "all_files_data_concat #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat.to_csv('dataSet/riss_bigdata.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title = all_files_data_concat['제목'] \n",
    "all_title #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "for title in all_title: \n",
    "    EnWords = re.sub(r\"[^a-zA-Z]+\", \" \", str(title)) \n",
    "    EnWordsToken = word_tokenize(EnWords.lower()) \n",
    "    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopWords] \n",
    "    EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop] \n",
    "    words.append(EnWordsTokenStopLemma)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = list(reduce(lambda x, y: x+y, words))\n",
    "print(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(words2)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "for tag, counts in count.most_common(50):\n",
    "    if(len(str(tag))>1):\n",
    "        word_count[tag] = counts\n",
    "        print(\"%s : %d\" % (tag, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_Keys = sorted(word_count, key = word_count.get, reverse = True)\n",
    "sorted_Values = sorted(word_count.values(), reverse = True)\n",
    "plt.bar(range(len(word_count)), sorted_Values, align = 'center')\n",
    "plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_count['big']\n",
    "del word_count['data']\n",
    "\n",
    "sorted_Keys = sorted(word_count, key = word_count.get, reverse = True)\n",
    "sorted_Values = sorted(word_count.values(), reverse = True)\n",
    "plt.bar(range(len(word_count)), sorted_Values, align = 'center')\n",
    "plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat['doc_count'] = 0 \n",
    "summary_year = all_files_data_concat.groupby('출판일', as_index = False)['doc_count'].count() \n",
    "summary_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"doc-count\")\n",
    "plt.grid(True)\n",
    "plt.plot(range(len(summary_year)), summary_year['doc_count'])\n",
    "plt.xticks(range(len(summary_year)), [text for text in summary_year['출판일']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wc = WordCloud(background_color = 'ivory', stopwords = stopwords, width = 1200, height = \n",
    "800)\n",
    "cloud = wc.generate_from_frequencies(word_count)\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud.to_file(\"dataSet/riss_bigdata_wordCloud.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
